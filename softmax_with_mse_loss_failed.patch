diff --git a/centerLossMNIST.py b/centerLossMNIST.py
index db278a7..769623d 100644
--- a/centerLossMNIST.py
+++ b/centerLossMNIST.py
@@ -27,17 +27,22 @@ class Net(nn.Module):
         self.out = self.fc3(x)
         x = self.fc2(self.out)
         # print("zeng>>", x, F.log_softmax(x, dim=1))
-        return F.log_softmax(x, dim=1), self.out
+        return F.softmax(x, dim=1), self.out


 def train(args, model, device, train_loader, optimizer, epoch):
     model.train()
     for batch_idx, (data, target) in enumerate(train_loader):
+        # print(target, target.view(64, -1))
+        target = torch.zeros(target.shape[0], 10).scatter_(1, target.view(target.shape[0], -1), 1)
+        # print("one:", target)
+
         # print(data.shape, target.shape, len(train_loader))
         data, target = data.to(device), target.to(device)
         optimizer.zero_grad()
         output, _ = model(data)
-        loss = F.nll_loss(output, target)
+        # print("debug:", output.shape, target.shape[0])
+        loss = F.mse_loss(output, target)
         loss.backward()
         optimizer.step()
         if batch_idx % args.log_interval == 0:
@@ -56,18 +61,19 @@ def test(args, model, device, test_loader):
     labels = []
     with torch.no_grad():
         for data, target in test_loader:
+            target = torch.zeros(target.shape[0], 10).scatter_(1, target.view(target.shape[0], -1), 1)
             data, target = data.to(device), target.to(device)
             output, center_out = model(data)
-            test_loss += F.nll_loss(output, target).item()  # sum up batch loss
+            test_loss += F.mse_loss(output, target).item()  # sum up batch loss
             pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability
-            correct += pred.eq(target.view_as(pred)).sum().item()
+            correct += pred.eq(target.max(1, keepdim=True)[1]).sum().item()
             # print(data.shape, data[0], target.shape, target[0])
             # print("out:", output.shape, output[0])
             # print("pred", pred.shape, pred[0])
             # print("xxxx\n", target.view_as(pred))
             # print("zzzz>>>\n", output.max(1, keepdim=True))
             center.append(center_out)
-            labels.append(target)
+            labels.append(target.max(1, keepdim=True)[1].view(-1))
     center_out = torch.cat(center, 0)
     target = torch.cat(labels, 0)
     center = center_out.data.cpu().numpy()
